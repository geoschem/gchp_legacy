#!/bin/bash

#SBATCH -n 96
#SBATCH -N 3
#SBATCH -t 0-24:00
#SBATCH -p huce_intel
#SBATCH --mem=MaxMemPerNode
#SBATCH --mail-type=ALL

# See SLURM documentation for descriptions of the above settings as well
# as many other settings you may use. For best performance, request all
# memory per node using --mem=MaxMemPerNode.

# Define GEOS-Chem log file
log="gchp.log"

# Always remove cap_restart if not doing a multi-segmented run.
if [[ -e cap_restart ]]; then
   rm cap_restart
fi

# Sync all config files with settings in runConfig.sh                           
./runConfig.sh > ${log}
if [[ $? == 0 ]]; then

    # Source your environment file. This requires first setting the gchp.env
    # symbolic link using script setEnvironment in the run directory. 
    # Be sure gchp.env points to the same file for both compilation and 
    # running. You can copy or adapt sample environment files located in 
    # ./envSamples subdirectory.
    gchp_env=$(readlink -f gchp.env)
    if [ ! -f ${gchp_env} ] 
    then
       echo "ERROR: gchp.rc symbolic link is not set!"
       echo "Copy or adapt an environment file from the ./envSamples "
       echo "subdirectory prior to running. Then set the gchp.env "
       echo "symbolic link to point to it using ./setEnvironment."
       echo "Exiting."
       exit 1
    fi
    echo " " >> ${log}
    echo "WARNING: You are using environment settings in ${gchp_env}" >> ${log}
    source ${gchp_env} >> ${log}

    # Use SLURM to distribute tasks across nodes
    NX=$( grep NX GCHP.rc | awk '{print $2}' )
    NY=$( grep NY GCHP.rc | awk '{print $2}' )
    coreCount=$(( ${NX} * ${NY} ))
    planeCount=$(( ${coreCount} / ${SLURM_NNODES} ))
    if [[ $(( ${coreCount} % ${SLURM_NNODES} )) > 0 ]]; then
	${planeCount}=$(( ${planeCount} + 1 ))
    fi

    # Echo info from computational cores to log file for displaying results
    echo "# of CPUs : ${coreCount}" >> ${log}
    echo "# of nodes: ${SLURM_NNODES}" >> ${log}
    echo "-m plane  : ${planeCount}" >> ${log}

    # Optionally compile 
    # Uncomment the line below to compile from scratch
    # See other compile options with 'make help'
    # make build_all

    # Echo start date
    echo ' ' >> ${log}
    echo '===> Run started at' `date` >> ${log}

    # If the shared memory option is on (USE_SHMEM: 1 in CAP.rc) then
    # clean up existing shared memory segments before and after running 
    # the model
    if grep -q "USE_SHMEM.*1" CAP.rc; then
        ipcs -a  # Show current shared memory segments
        ipcs -l  # Show shared memory limits
        /sbin/sysctl -A | grep shm
        memCleaner=./runScriptSamples/rmshmem.sh
        srun -w $SLURM_JOB_NODELIST -n $SLURM_NNODES --ntasks-per-node=1 ${memCleaner}
        ipcs -a  # Show current shared memory segments again (should be none)
    fi

    # Odyssey-specific setting to get around connection issues at high # cores
    export OMPI_MCL_btl=openib

    # Start the simulation
    time srun -n ${coreCount} -N ${SLURM_NNODES} -m plane=${planeCount} --mpi=pmix ./geos >> ${log}

    # Clean up any shared memory segments owned by this user
    if grep -q "USE_SHMEM.*1" CAP.rc; then
        srun -w $SLURM_JOB_NODELIST -n $SLURM_NNODES --ntasks-per-node=1 ${memCleaner}
    fi

    # Echo end date
    echo '===> Run ended at' `date` >> ${log}

else
    cat ${log}
fi

# Clear variable
unset log

# Exit normally
exit 0

